{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7EXnt9uUSv5",
        "outputId": "6572f6df-f173-42b3-80b6-049c931efec7"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "\"\"\"\n",
        "- Script name: spacy_models_evaluate\n",
        "- Author: Dan Bright, cosmoid@tuta.io\n",
        "- Description: A script to evaluate performance of spaCy \n",
        "  language models for NER\n",
        "\"\"\"\n",
        "\n",
        "# install packages\n",
        "#!pipenv install matplotlib spacy numpy pandas spacy_stanza spacy-transformers   # pipenv environment\n",
        "!pip install matplotlib spacy numpy pandas spacy_stanza spacy-transformers   # Google colab or other regular pip environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uesVopp8ZsoB",
        "outputId": "36f0aefd-e712-4a0a-ec2e-a85276f4c37a"
      },
      "outputs": [],
      "source": [
        "#!python -m spacy download en_core_web_lg \n",
        "!python -m spacy download en_core_web_trf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ2KMXaXVBEM"
      },
      "outputs": [],
      "source": [
        "def mount_google_drive():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPQc5ddUUSv6"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import spacy, json, copy, glob, os, matplotlib, stanza, spacy_stanza, spacy_transformers\n",
        "import pandas as pd\n",
        "from spacy import displacy\n",
        "import locale\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"  # Fix Colab local bug\n",
        "\n",
        "def get_annotation_urls(url, ext, print_output=True):\n",
        "    global annotated_files\n",
        "    # ensure url has trailing slash\n",
        "    url = url + '/' if url[-1:] != '/' else url\n",
        "    # load hand annotated examples\n",
        "    annotated_files = glob.glob(url + f'*.{ext}')\n",
        "    # sort based on filename\n",
        "    annotated_files.sort(key=lambda x: os.path.basename(x))\n",
        "    # print counted files to demonstrate success\n",
        "    if print_output:\n",
        "        print(f'Number of annotated files: {len(annotated_files)}')\n",
        "\n",
        "def json_to_doc(print_output=False):\n",
        "    # Load json into list of Python dicts\n",
        "    global annotations\n",
        "    annotations = []\n",
        "    if print_output:\n",
        "      print(annotated_files)\n",
        "    for idx, f in enumerate(annotated_files):\n",
        "        print(f'Opening file: {idx}')\n",
        "        with open(f, 'r', encoding='utf-8') as file:\n",
        "            annotations.append(json.loads(file.read()))\n",
        "    if print_output:\n",
        "        # print count of annotation dicts to verify success\n",
        "        print(f'Number of annotations in files: {len(annotations)}')\n",
        "        # print first element (document), to verify\n",
        "        print(f'Annotation sample: {annotations[:1]}')\n",
        "\n",
        "def load_models(print_output, model):\n",
        "    global nlp, nlp_trf_orig\n",
        "    # load models\n",
        "    if model == 'stanza':\n",
        "        stanza.download(\"en\")\n",
        "        nlp = spacy_stanza.load_pipeline(\"en\")\n",
        "    elif model in ['trf-model-best', 'trf-model-best-tuned', 'cnn-model-best']:\n",
        "        nlp = spacy.load(trained_model_path + model, exclude='parser,tagger,attribute_ruler,lemmatizer')\n",
        "        nlp_trf_orig = spacy.load('en_core_web_trf' if model in ['trf-model-best', 'trf-model-best-tuned'] else 'en_core_web_lg')  # required as workaround to frozen components bug\n",
        "        nlp.add_pipe('parser', source=nlp_trf_orig, after='transformer' if model in ['trf-model-best', 'trf-model-best-tuned'] else 'tok2vec')\n",
        "        nlp.add_pipe('tagger', source=nlp_trf_orig, after='parser')\n",
        "        nlp.add_pipe('attribute_ruler', source=nlp_trf_orig, after='tagger')\n",
        "        nlp.add_pipe('lemmatizer', source=nlp_trf_orig, after='attribute_ruler')\n",
        "        print(f'Evaluating model: {model}') if print_output else None\n",
        "    else:\n",
        "        model = trained_model_path + model if model not in ['en_core_web_lg', 'en_core_web_trf'] else model\n",
        "        print(f'Evaluating model: {model}') if print_output else None\n",
        "        nlp = spacy.load(model)\n",
        "    \n",
        "\n",
        "def setup(model, dataset, print_output=False, colab=0):\n",
        "    # set up paths etc.\n",
        "    global trained_model_path, doc_results_path, doc_results_filename, corpus_results_path, corpus_results_filename\n",
        "    \"\"\"paths\"\"\"\n",
        "    mount_google_drive() if colab else None # mount google drive if colab boolean True\n",
        "    google_drive_path = '/content/drive/MyDrive/'  # path on google drive to a data directory\n",
        "    annotations_path = f'{google_drive_path if colab else \"./\"}data/{dataset}/json/' # path to annotations\n",
        "    trained_model_path = f'{google_drive_path if colab else \"./\"}models/'  # path to the model\n",
        "    doc_results_path = f'{google_drive_path if colab else \"./\"}testing_results/'  # path for tested documents results\n",
        "    doc_results_filename = f'doc_{model}.csv'  # filename for tested documents results\n",
        "    corpus_results_path = f'{google_drive_path if colab else \"./\"}test_results/'  # path for corpus results\n",
        "    corpus_results_filename = f'corpus_{model}.csv'  # filename for corpus results\n",
        "    annotations_data_filetype = 'json'\n",
        "    \"\"\"globals set here\"\"\"\n",
        "    global annotated_files, labels_of_interest\n",
        "    # define entity labels of interest\n",
        "    labels_of_interest = ['ATC_CITY', 'ATC_STATE', 'ICDT_DATE', 'ICDT_TIME', 'ICDT_LOC', 'UAS_COLOR', 'UAS_SHAPE',\n",
        "                          'UAS_HEADING', 'UAS_SIZE', 'UAS_REL_ALT', 'UAS_ACT_ALT', 'AC_ALT', 'AC_TYPE', 'AC_HEADING', 'FT_NAME','FL_OPTOR','FT_ROUTE']\n",
        "    # run setup functions\n",
        "    get_annotation_urls(annotations_path, annotations_data_filetype, print_output)\n",
        "    json_to_doc(print_output)\n",
        "    load_models(print_output, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgnX6E4xUSv7"
      },
      "outputs": [],
      "source": [
        "def get_annotated_entities(annotations):\n",
        "    # function to get all entities in a hand-annotated doc (all paras)\n",
        "    return [para[1]['entities'] for para in annotations if para]\n",
        "\n",
        "def get_text(annotations):\n",
        "    # function to get all raw text from the hand-annotated doc (all paras)\n",
        "    return [para[0] for para in annotations if para]\n",
        "\n",
        "def get_annotations(print_output=False):\n",
        "    global annotated_entities, annotated_text\n",
        "    \"\"\"Note: Entities to be stored in the form [[[element1, element2]],[[element1, element2]]]\n",
        "    \"\"\"\n",
        "    # run function to get all entities from all paras in all the passed-in hand-annotated docs\n",
        "    annotated_entities = [get_annotated_entities(doc['annotations']) for doc in annotations]\n",
        "    # run function to get all text from all paras in all the passed-in hand-annotated docs\n",
        "    annotated_text = [get_text(doc['annotations']) for doc in annotations]\n",
        "    \"\"\"Note: Annotated text stored in form [[para1, para1],[para1, para2]]\n",
        "    i.e., a list of document-lists of paras\n",
        "    \"\"\"\n",
        "    if print_output:\n",
        "        # print total counts of annotated documents; paras and entities\n",
        "        print(f'Number of documents: {len(annotated_entities)}')\n",
        "        print(f'Number of paras: {sum([len(x) for x in annotated_entities])}')\n",
        "        print(f'Number of entities: {sum([sum(len(y) for y in x ) for x in annotated_entities])}\\n')\n",
        "        # print first entity, of first para, of first doc, to verify entities\n",
        "        print(f'Annotated entities sample (doc 1, para 1): {annotated_entities[0][0]}\\n')\n",
        "         # print sample of annotated text to verify\n",
        "        print(f'Annotated text sample (doc 1, para 1): {annotated_text[0][0]}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjO5Bpw_USv7"
      },
      "outputs": [],
      "source": [
        "def run_ner(model, print_output=False):\n",
        "    global extracted_entities\n",
        "    extracted_entities = [] # in the form [[[[element1, element2]],[doc2[element1, element2]]]]\n",
        "    for doc_text in annotated_text:\n",
        "        doc_ents = []\n",
        "        for para in nlp.pipe(doc_text, disable=['tagger', 'parser', 'attribute_ruler', 'lemmatizer']):\n",
        "            doc_ents.append([[ent.start_char, ent.end_char, ent.label_] for ent in para.ents])  # could also add ent.text\n",
        "        extracted_entities.append(doc_ents)\n",
        "    \n",
        "    if print_output:\n",
        "        # print total counts of processed documents; paras and entities\n",
        "        print(f'Number of documents: {len(extracted_entities)}')\n",
        "        print(f'Number of paras: {sum([len(x) for x in extracted_entities])}')\n",
        "        print(f'Number of entities: {sum([sum(len(y) for y in x ) for x in extracted_entities])}')\n",
        "        print('\\n')\n",
        "        print(f'Extract entities: {extracted_entities}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4UTT0-XUSv8"
      },
      "outputs": [],
      "source": [
        "# function to count annotated and extracted entities in doc\n",
        "def count_entities(doc):\n",
        "    return sum([len(list(ent for ent in para)) for para in doc]) if any(doc) else 0\n",
        "\n",
        "def count_sample_paragraphs():\n",
        "    return sum([len(doc['annotations']) for doc in annotations])\n",
        "\n",
        "def count_entity_types(dataset='annotations'):\n",
        "    global entity_count\n",
        "    entity_count = {k:0 for k in labels_of_interest}\n",
        "    for doc in annotated_entities if dataset == 'annotations' else extracted_entities:\n",
        "        for para in doc:\n",
        "            for ent in para:\n",
        "                entity_count[ent[2]] += 1\n",
        "    return entity_count\n",
        "\n",
        "\"\"\" run the count routines\n",
        "\"\"\"\n",
        "def run_counts(print_output=False):\n",
        "    global doc_extracted_entities_count, doc_annotated_entities_count, corpus_extracted_entities_count, corpus_annotated_entities_count, extracted_entities, annotated_entities, corpus_sample_paras_total\n",
        "    # count all entities for each doc\n",
        "    doc_extracted_entities_count = [count_entities(doc) for doc in extracted_entities]\n",
        "    doc_annotated_entities_count = [count_entities(doc) for doc in annotated_entities]\n",
        "    # count all sample paragraphs for corpus\n",
        "    corpus_sample_paras_total = count_sample_paragraphs()\n",
        "    # count all entities for corpus\n",
        "    corpus_extracted_entities_count = sum(doc_extracted_entities_count)\n",
        "    corpus_annotated_entities_count = sum(doc_annotated_entities_count)\n",
        "    # print output (always)\n",
        "    print(f'\\nExtracted entities count for each doc: {doc_extracted_entities_count}\\nAnnotated entities count for each doc: {doc_annotated_entities_count}')\n",
        "    print(f'\\nTotal annotated sample paragraphs in the corpus: {corpus_sample_paras_total}\\n')\n",
        "    print(f'Extracted entities count for corpus: {corpus_extracted_entities_count}')\n",
        "    print(f'Annotated entities count for corpus: {corpus_annotated_entities_count}\\n')\n",
        "    print(f'Totals of annotated entity types: {count_entity_types(\"annotations\")}')\n",
        "    print(f'Totals of extracted entity types: {count_entity_types(\"ner\")}')\n",
        "    # print output (optional)\n",
        "    if print_output:\n",
        "        print(f'\\nExtracted entities for corpus: {extracted_entities}')\n",
        "        print(f'Annotated entities for corpus: {annotated_entities}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxOB7DthUSv8"
      },
      "outputs": [],
      "source": [
        "# function to remove any entities not of interest in doc (ensures extra detected entities do not influence performance calculations and useful for calculating performance without influence of custom entities)\n",
        "def remove_irrelevant_entities(doc):\n",
        "    for idx, para in enumerate(doc):\n",
        "        doc[idx] = [ent for ent in para if ent[2] in labels_of_interest]\n",
        "    return doc\n",
        "\n",
        "# perform preprocessing on the data\n",
        "def preprocess_data(print_output=False):\n",
        "    global extracted_entities, annotated_entities, annotated_text, test_data\n",
        "    # extract irrelevant entities\n",
        "    extracted_entities = [remove_irrelevant_entities(doc) for doc in extracted_entities]\n",
        "    annotated_entities = [remove_irrelevant_entities(doc) for doc in annotated_entities]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBrl8eeqUSv8"
      },
      "outputs": [],
      "source": [
        "def analyse(print_output=False):\n",
        "\n",
        "    global doc_extracted_entities_count, doc_annotated_entities_count, corpus_extracted_entities_count, corpus_annotated_entities_count, extracted_entities, corpus_false_positive_entities, true_positives, false_negatives, corpus_true_positives_total, doc_results\n",
        "\n",
        "    def find_matches(print_output, doc_idx, annotated, extracted):\n",
        "        # find true positives and false negatives, per document\n",
        "        true_pos = 0\n",
        "        false_neg = doc_annotated_entities_count[doc_idx] - doc_extracted_entities_count[doc_idx]\n",
        "        for para_idx, (a_ents, e_ents) in enumerate(zip(annotated, extracted)):\n",
        "            ee_matched = []\n",
        "            ae_matched = []\n",
        "            # look for extracted entities within annotated entity boundaries (matches)\n",
        "            for ae in a_ents:\n",
        "                for ee in e_ents:\n",
        "                    if (ee[1] >= ae[0] and ee[0] <= ae[1]):\n",
        "                        if ee[2] == ae[2]: # true positive identified!\n",
        "                            if ee in ee_matched: \n",
        "                                false_neg -= 1  # decrement false negatives by 1\n",
        "                            else:\n",
        "                                ee_matched.append(ee)  # record NER identified entity as seen\n",
        "                                true_pos += 1  # increment true positives by 1\n",
        "                            if ae in ae_matched:\n",
        "                                false_neg += 1  # increment false negatives by 1\n",
        "                            else:\n",
        "                                ae_matched.append(ae)  # record appended entity as seen\n",
        "        # Prevent false_neg falling below 0 in event that \n",
        "        #  more NER entities than annotated unduly influence this calculation\n",
        "        false_neg = false_neg if false_neg > 0 else 0           \n",
        "        return true_pos, false_neg\n",
        "\n",
        "    def calc_corpus_true_positives_total(print_output):\n",
        "        # function to calculate true positives for the corpus\n",
        "        global corpus_true_positives_total\n",
        "        corpus_true_positives_total = sum(true_positives)\n",
        "\n",
        "    def calc_corpus_false_negatives_total(print_output):\n",
        "        # function to calculate false negatives for the corpus\n",
        "        global corpus_false_negatives_total\n",
        "        corpus_false_negatives_total = sum(false_negatives)\n",
        "    \n",
        "    def calc_true_pos_false_neg(print_output):\n",
        "        global true_positives, false_negatives\n",
        "        true_positives = []\n",
        "        false_negatives = []\n",
        "        for doc_idx, doc in enumerate(annotated_entities):\n",
        "            true_pos, false_neg = find_matches(print_output, doc_idx, annotated_entities[doc_idx],extracted_entities[doc_idx])\n",
        "            true_positives.append(true_pos)\n",
        "            false_negatives.append(false_neg)\n",
        "\n",
        "    # functions to compute precision, recall and f1-score for model-level evaluation\n",
        "\n",
        "    def model_level_eval_doc():\n",
        "        global doc_results\n",
        "        doc_results = []\n",
        "        precision_list = []\n",
        "        recall_list = []\n",
        "        f1_score_list = []\n",
        "         \n",
        "        for tp,fn,ee in zip(true_positives,false_negatives,doc_extracted_entities_count):\n",
        "            # calculate precision for each doc\n",
        "            precision_list.append(tp/ee) if tp > 0 else precision_list.append(1.0) if (fn == 0 and ee == 0) else precision_list.append(0)\n",
        "             # calculate recall for each doc\n",
        "            recall_list.append(tp/(tp + fn)) if tp > 0 else recall_list.append(1.0) if (fn == 0 and ee == 0) else recall_list.append(0)\n",
        "        for idx, (p,r,tp,fn,ee) in enumerate(zip(precision_list,recall_list,true_positives,false_negatives,doc_extracted_entities_count)):\n",
        "             # calculate f1-score for each doc\n",
        "            f1_score_list.append((2 * p * r / (p + r))) if (p > 0 and r > 0) else f1_score_list.append(1.0) if (tp == 0 and (ee == 0 and fn == 0)) else f1_score_list.append(0)\n",
        "        # add results to results dictionary\n",
        "        for doc in range(len(precision_list)):\n",
        "            doc_results.append({\n",
        "                'precision': round(precision_list[doc], 3), \n",
        "                'recall': round(recall_list[doc], 3),\n",
        "                'f1-score': round(f1_score_list[doc], 3)\n",
        "                })\n",
        "\n",
        "    def model_level_eval_corpus():\n",
        "        global corpus_results\n",
        "        # calculate precision of corpus\n",
        "        precision = corpus_true_positives_total/corpus_extracted_entities_count if corpus_true_positives_total >  0 else 1.0 if (corpus_extracted_entities_count == 0 and corpus_false_negatives_total == 0) else 0\n",
        "        # calculate recall for each doc\n",
        "        recall = corpus_true_positives_total/(corpus_true_positives_total + corpus_false_negatives_total) if corpus_true_positives_total > 0 else 1.0 if (corpus_extracted_entities_count == 0 and corpus_false_negatives_total == 0) else 0\n",
        "        # calculate f1-score for each doc\n",
        "        f1_score = 2 * precision * recall / (precision + recall) if (precision > 0 and recall > 0) else 1.0 if corpus_true_positives_total == 0 and (corpus_extracted_entities_count == 0 and corpus_false_negatives_total == 0) else 0\n",
        "        corpus_results = {\n",
        "            'precision': round(precision, 3), \n",
        "            'recall': round(recall, 3), \n",
        "            'f1_score':round(f1_score, 3)\n",
        "            }\n",
        "    \n",
        "    # run the tasks\n",
        "    corpus_false_positive_entities = []\n",
        "    corpus_missed_entities = []\n",
        "    corpus_false_positive_entities.clear() # clear list\n",
        "    corpus_missed_entities.clear() # clear list first\n",
        "    preprocess_data(print_output=0)  # create working copies of data\n",
        "    run_counts(0)\n",
        "    calc_true_pos_false_neg(print_output)\n",
        "    calc_corpus_true_positives_total(print_output)\n",
        "    calc_corpus_false_negatives_total(print_output)\n",
        "    model_level_eval_doc()\n",
        "    model_level_eval_corpus()\n",
        "\n",
        "    if print_output:\n",
        "        print('\\n')\n",
        "        # print totals per doc\n",
        "        for doc_idx, doc in enumerate(extracted_entities):\n",
        "            print(f'Document {doc_idx}')\n",
        "            print(f'True positives: {true_positives[doc_idx]}')\n",
        "            print(f'False positives: {doc_extracted_entities_count[doc_idx] - true_positives[doc_idx]}')\n",
        "            print(f'False negatives: {false_negatives[doc_idx]}\\n')\n",
        "        # print total for all docs\n",
        "        print(f'\\nDocument analysis results: {doc_results}')\n",
        "        print(f'Corpus analysis results {corpus_results}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbOueK6gUSv9"
      },
      "outputs": [],
      "source": [
        "def display_examples(docs_of_interest):\n",
        "    for doc in docs_of_interest:\n",
        "        try:\n",
        "            for para_idx, para in enumerate(annotated_entities[doc]):\n",
        "                ae = annotated_entities[doc][para_idx]\n",
        "                text = annotated_text[doc][para_idx]\n",
        "                print(f'\\nAnnotated example for document {doc}')\n",
        "                displacy.render({\n",
        "                'text': text,\n",
        "                'ents': [{\"start\": e[0], \"end\": e[1], \"label\": e[2]} for e in ae],\n",
        "                \"title\": f'Document {doc}, para {para_idx}'\n",
        "            }, manual=True, style='ent', jupyter=True)\n",
        "                print(f'\\nExtracted example for document {doc}')\n",
        "                ee = extracted_entities[doc][para_idx]\n",
        "                displacy.render({\n",
        "                'text': text,\n",
        "                'ents': [{\"start\": e[0], \"end\": e[1], \"label\": e[2]} for e in ee],\n",
        "                \"title\": f'Document {doc}, para {para_idx}'\n",
        "            }, manual=True, style='ent', jupyter=True)\n",
        "        except IndexError:\n",
        "            print(f'You appear to be trying to display results for document {doc}, which does not appear to exist!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFGW4K0kUSv-"
      },
      "outputs": [],
      "source": [
        "def display_results(show_label_examples=[], show_scores=True):\n",
        "    global df_docs_results, df_corpus_results\n",
        "    if show_label_examples:\n",
        "        display_examples(show_label_examples)\n",
        "    if show_scores:\n",
        "        # convert dict to pandas dataframe, for display\n",
        "        dr = dict()\n",
        "        for idx, doc in enumerate(doc_results):\n",
        "            dr[idx] = {k.capitalize(): v for k,v in doc.items()}\n",
        "        print('\\nDocument Analysis Results')\n",
        "        df_docs_results = pd.DataFrame.from_dict(dr)\n",
        "        df_docs_results = df_docs_results.T\n",
        "        df_docs_results = df_docs_results[['Precision','Recall','F1-score']]\n",
        "        df_docs_results.index.name = 'Document'\n",
        "        display(df_docs_results)\n",
        "        print('\\nCorpus Analysis Results')\n",
        "        cr = {k.capitalize():[v] for k, v in corpus_results.items()}\n",
        "        df_corpus_results = pd.DataFrame.from_dict(cr)\n",
        "        df_corpus_results.index.name = 'Corpus'\n",
        "        display(df_corpus_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWka_K6eUSv-"
      },
      "outputs": [],
      "source": [
        "def write_results_to_file(write):\n",
        "    if write:\n",
        "        df_docs_results.to_csv(doc_results_path + doc_results_filename)\n",
        "        df_corpus_results.to_csv(corpus_results_path + corpus_results_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Bz7V9wMUSv-",
        "outputId": "b4c4d1de-b745-4bfa-e275-8e6079032086"
      },
      "outputs": [],
      "source": [
        "global model\n",
        "'''run processes'''\n",
        "model = 'trf-model-best' # model options: trf-model-best\n",
        "setup(model=model, dataset='test', print_output=1, colab=1)  # dataset from 'train', 'test'. colab=1 if run on colab\n",
        "get_annotations(print_output=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1h6oP07Xal8",
        "outputId": "97d64827-1973-4d62-e7e0-2aa2ec1a1774"
      },
      "outputs": [],
      "source": [
        "run_ner(model, print_output=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q3NWbPf7Xa6g",
        "outputId": "6bf84210-9888-4175-fe73-c28c31991350"
      },
      "outputs": [],
      "source": [
        "analyse(print_output=1) \n",
        "# args: ([Indexes (counting from 1) of docs to display entities for (list)], show scores? (boolean))\n",
        "display_results([0],1)\n",
        "write_results_to_file(0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.14 ('uhi-ZQVV2iWc')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "65f88eacf3a10b22e2367da6754b23494b2804a02fe03c23afbe72788a968f5d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
